mtime:1741376717.453172
{"name": "RPA.Cloud.Azure", "doc": "Azure is a library for operating with Microsoft Azure API endpoints.\n\nList of supported service names:\n\n* computervision ([Azure Computer Vision API](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/))\n\n* face ([Azure Face API](https://docs.microsoft.com/en-us/azure/cognitive-services/face/))\n\n* speech ([Azure Speech Services API](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/))\n\n* textanalytics ([Azure Text Analytics API](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/))\n\n**Azure authentication**\n\nAuthentication for Azure is set with service subscription key which can be given to the library\nin two different ways.\n\n* Method 1 as environment variables, either service specific environment variable\n  for example `AZURE_TEXTANALYTICS_KEY` or with common key `AZURE_SUBSCRIPTION_KEY` which\n  will be used for all the services.\n\n* Method 2 as Robocorp Vault secret. The vault name needs to be given in library init or\n  with keyword `Set Robocorp Vault`. Secret keys are expected to match environment variable\n  names.\n\nMethod 1. subscription key using environment variable\n\n```robotframework\n*** Settings ***\nLibrary   RPA.Cloud.Azure\n\n*** Tasks ***\nInit Azure services\n    # NO parameters for client, expecting to get subscription key\n    # with AZURE_TEXTANALYTICS_KEY or AZURE_SUBSCRIPTION_KEY environment variable\n    Init Text Analytics Service\n```\n\nMethod 2. setting Robocorp Vault in the library init\n\n```robotframework\n*** Settings ***\nLibrary   RPA.Cloud.Azure  robocorp_vault_name=azure\n\n*** Tasks ***\nInit Azure services\n    Init Text Analytics Service  use_robocorp_vault=${TRUE}\n```\n\nMethod 2. setting Robocorp Vault with keyword\n\n```robotframework\n*** Settings ***\nLibrary   RPA.Cloud.Azure\n\n*** Tasks ***\nInit Azure services\n    Set Robocorp Vault          vault_name=googlecloud\n    Init Text Analytics Service  use_robocorp_vault=${TRUE}\n```\n\n**References**\n\nList of supported language locales - [Azure locale list](https://docs.microsoft.com/en-gb/azure/cognitive-services/speech-service/language-support#speech-to-text)\n\nList of supported region identifiers - [Azure region list](https://docs.microsoft.com/en-gb/azure/cognitive-services/speech-service/regions#speech-to-text-text-to-speech-and-translation)\n\n**Examples**\n\n**Robot Framework**\n\nThis is a section which describes how to use the library in your\nRobot Framework tasks.\n\n```robotframework\n*** Settings ***\nLibrary  RPA.Cloud.Azure\n\n*** Variables ***\n${IMAGE_URL}   IMAGE_URL\n${FEATURES}    Faces,ImageType\n\n*** Tasks ***\nVisioning image information\n   Init Computer Vision Service\n   &{result}   Vision Analyze  image_url=${IMAGE_URL}  visual_features=${FEATURES}\n   @{faces}    Set Variable  ${result}[faces]\n   FOR  ${face}  IN   @{faces}\n      Log  Age: ${face}[age], Gender: ${face}[gender], Rectangle: ${face}[faceRectangle]\n   END\n```\n\n**Python**\n\nThis is a section which describes how to use the library in your\nown Python modules.\n\n```python\nlibrary = Azure()\nlibrary.init_text_analytics_service()\nlibrary.init_face_service()\nlibrary.init_computer_vision_service()\nlibrary.init_speech_service(\"westeurope\")\n\nresponse = library.sentiment_analyze(\n   text=\"The rooms were wonderful and the staff was helpful.\"\n)\nresponse = library.detect_face(\n   image_file=PATH_TO_FILE,\n   face_attributes=\"age,gender,smile,hair,facialHair,emotion\",\n)\nfor item in response:\n   gender = item[\"faceAttributes\"][\"gender\"]\n   age = item[\"faceAttributes\"][\"age\"]\n   print(f\"Detected a face, gender:{gender}, age: {age}\")\n\nresponse = library.vision_analyze(\n   image_url=URL_TO_IMAGE,\n   visual_features=\"Faces,ImageType\",\n)\nmeta = response['metadata']\nprint(\n   f\"Image dimensions meta['width']}x{meta['height']} pixels\"\n)\n\nfor face in response[\"faces\"]:\n   left = face[\"faceRectangle\"][\"left\"]\n   top = face[\"faceRectangle\"][\"top\"]\n   width = face[\"faceRectangle\"][\"width\"]\n   height = face[\"faceRectangle\"][\"height\"]\n   print(f\"Detected a face, gender:{face['gender']}, age: {face['age']}\")\n   print(f\"      Face rectangle: (left={left}, top={top})\")\n   print(f\"      Face rectangle: (width={width}, height={height})\")\n\nlibrary.text_to_speech(\n    text=\"Developer tools for open-source RPA leveraging the Robot Framework ecosystem\",\n    neural_voice_style=\"cheerful\",\n    target_file='output.mp3'\n)\n```\n", "version": null, "specversion": "6", "type": "LIBRARY", "scope": "GLOBAL", "docFormat": "markdown", "source": "C:\\Users\\Gustavo Zanotto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\RPA\\Cloud\\Azure.py", "lineno": 591, "tags": [], "inits": [{"name": "__init__", "args": [{"name": "region", "kind": "POSITIONAL_OR_NAMED", "repr": "region: str = northeurope", "required": true, "defaultValue": "northeurope", "types": ["str"], "typedocs": []}, {"name": "robocorp_vault_name", "kind": "POSITIONAL_OR_NAMED", "repr": "robocorp_vault_name: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Initialize self.  See help(type(self)) for accurate signature.\n", "tags": [], "source": null, "shortdoc": "Initialize self.  See help(type(self)) for accurate signature.", "lineno": 740}], "keywords": [{"name": "Detect Face", "args": [{"name": "image_file", "kind": "POSITIONAL_OR_NAMED", "repr": "image_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "image_url", "kind": "POSITIONAL_OR_NAMED", "repr": "image_url: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "face_attributes", "kind": "POSITIONAL_OR_NAMED", "repr": "face_attributes: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "face_landmarks", "kind": "POSITIONAL_OR_NAMED", "repr": "face_landmarks: bool = False", "required": true, "defaultValue": "False", "types": ["bool"], "typedocs": []}, {"name": "recognition_model", "kind": "POSITIONAL_OR_NAMED", "repr": "recognition_model: str = recognition_02", "required": true, "defaultValue": "recognition_02", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Detect facial attributes in the image\n\nparam image_file\n\n:   filepath of image file\n\nparam image_url\n\n:   URI to image, if given will be used instead of image\\_file\n\nparam face_attributes\n\n:   comma separated list of attributes,\n    for example. \"age,gender,smile\"\n\nparam face_landmarks\n\n:   return face landmarks of the detected faces\n    or not. The default value is False\n\nparam recognition_model\n\n:   model used by Azure to detech faces, options\n    are \"recognition_01\" or \"recognition_02\", default is \"recognition_02\"\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n\nRead more about face\\_attributes at [Face detection explained](https://docs.microsoft.com/en-us/azure/cognitive-services/face/concepts/face-detection):\n\n* age\n\n* gender\n\n* smile\n\n* facialHair\n\n* headPose\n\n* glasses\n\n* emotion\n\n* hair\n\n* makeup\n\n* accessories\n\n* blur\n\n* exposure\n\n* nouse\n", "tags": [], "source": null, "shortdoc": "Detect facial attributes in the image", "lineno": 294}, {"name": "Detect Language", "args": [{"name": "text", "kind": "POSITIONAL_OR_NAMED", "repr": "text: str", "required": true, "defaultValue": null, "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Detect languages in the given text\n\nparam text\n\n:   A UTF-8 text string\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n", "tags": [], "source": null, "shortdoc": "Detect languages in the given text", "lineno": 223}, {"name": "Find Entities", "args": [{"name": "text", "kind": "POSITIONAL_OR_NAMED", "repr": "text: str", "required": true, "defaultValue": null, "types": ["str"], "typedocs": []}, {"name": "language", "kind": "POSITIONAL_OR_NAMED", "repr": "language: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file=None", "required": true, "defaultValue": "None", "types": [], "typedocs": []}], "doc": "Detect entities in the given text\n\nparam text\n\n:   A UTF-8 text string\n\nparam language\n\n:   if input language is known\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n", "tags": [], "source": null, "shortdoc": "Detect entities in the given text", "lineno": 256}, {"name": "Init Computer Vision Service", "args": [{"name": "region", "kind": "POSITIONAL_OR_NAMED", "repr": "region: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "use_robocorp_vault", "kind": "POSITIONAL_OR_NAMED", "repr": "use_robocorp_vault: bool = False", "required": true, "defaultValue": "False", "types": ["bool"], "typedocs": []}], "doc": "Initialize Azure Computer Vision\n\nparam region\n\n:   identifier for service region\n\nparam use_robocorp_vault\n\n:   use secret stored into Robocorp Vault\n", "tags": [], "source": null, "shortdoc": "Initialize Azure Computer Vision", "lineno": 359}, {"name": "Init Face Service", "args": [{"name": "region", "kind": "POSITIONAL_OR_NAMED", "repr": "region: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "use_robocorp_vault", "kind": "POSITIONAL_OR_NAMED", "repr": "use_robocorp_vault: bool = False", "required": true, "defaultValue": "False", "types": ["bool"], "typedocs": []}], "doc": "Initialize Azure Face\n\nparam region\n\n:   identifier for service region\n\nparam use_robocorp_vault\n\n:   use secret stored into Robocorp Vault\n", "tags": [], "source": null, "shortdoc": "Initialize Azure Face", "lineno": 282}, {"name": "Init Speech Service", "args": [{"name": "region", "kind": "POSITIONAL_OR_NAMED", "repr": "region: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "use_robocorp_vault", "kind": "POSITIONAL_OR_NAMED", "repr": "use_robocorp_vault: bool = False", "required": true, "defaultValue": "False", "types": ["bool"], "typedocs": []}], "doc": "Initialize Azure Speech\n\nparam region\n\n:   identifier for service region\n\nparam use_robocorp_vault\n\n:   use secret stored into Robocorp Vault\n", "tags": [], "source": null, "shortdoc": "Initialize Azure Speech", "lineno": 478}, {"name": "Init Text Analytics Service", "args": [{"name": "region", "kind": "POSITIONAL_OR_NAMED", "repr": "region: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "use_robocorp_vault", "kind": "POSITIONAL_OR_NAMED", "repr": "use_robocorp_vault: bool = False", "required": true, "defaultValue": "False", "types": ["bool"], "typedocs": []}], "doc": "Initialize Azure Text Analyticts\n\nparam region\n\n:   identifier for service region\n\nparam use_robocorp_vault\n\n:   use secret stored into Robocorp Vault\n", "tags": [], "source": null, "shortdoc": "Initialize Azure Text Analyticts", "lineno": 191}, {"name": "Key Phrases", "args": [{"name": "text", "kind": "POSITIONAL_OR_NAMED", "repr": "text: str", "required": true, "defaultValue": null, "types": ["str"], "typedocs": []}, {"name": "language", "kind": "POSITIONAL_OR_NAMED", "repr": "language: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Detect key phrases in the given text\n\nparam text\n\n:   A UTF-8 text string\n\nparam language\n\n:   if input language is known\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n", "tags": [], "source": null, "shortdoc": "Detect key phrases in the given text", "lineno": 237}, {"name": "List Supported Voices", "args": [{"name": "locale", "kind": "POSITIONAL_OR_NAMED", "repr": "locale: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "neural_only", "kind": "POSITIONAL_OR_NAMED", "repr": "neural_only: bool = False", "required": true, "defaultValue": "False", "types": ["bool"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "List supported voices for Azure API Speech Services.\n\nparam locale\n\n:   list only voices specific to locale, by default return all voices\n\nparam neural_only\n\n:   True if only neural voices should be returned,\n    False by default\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   voices in json\n\nAvailable voice selection might differ between regions.\n", "tags": [], "source": null, "shortdoc": "List supported voices for Azure API Speech Services.", "lineno": 564}, {"name": "Sentiment Analyze", "args": [{"name": "text", "kind": "POSITIONAL_OR_NAMED", "repr": "text: str", "required": true, "defaultValue": null, "types": ["str"], "typedocs": []}, {"name": "language", "kind": "POSITIONAL_OR_NAMED", "repr": "language: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Analyze sentiments in the given text\n\nparam text\n\n:   A UTF-8 text string\n\nparam language\n\n:   if input language is known\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n", "tags": [], "source": null, "shortdoc": "Analyze sentiments in the given text", "lineno": 203}, {"name": "Set Robocorp Vault", "args": [{"name": "vault_name", "kind": "POSITIONAL_OR_NAMED", "repr": "vault_name", "required": true, "defaultValue": null, "types": [], "typedocs": []}], "doc": "Set Robocorp Vault name\n\nparam vault_name\n\n:   Robocorp Vault name\n", "tags": [], "source": null, "shortdoc": "Set Robocorp Vault name", "lineno": 173}, {"name": "Text To Speech", "args": [{"name": "text", "kind": "POSITIONAL_OR_NAMED", "repr": "text: str", "required": true, "defaultValue": null, "types": ["str"], "typedocs": []}, {"name": "language", "kind": "POSITIONAL_OR_NAMED", "repr": "language: str = en-US", "required": true, "defaultValue": "en-US", "types": ["str"], "typedocs": []}, {"name": "name", "kind": "POSITIONAL_OR_NAMED", "repr": "name: str = en-US-AriaRUS", "required": true, "defaultValue": "en-US-AriaRUS", "types": ["str"], "typedocs": []}, {"name": "gender", "kind": "POSITIONAL_OR_NAMED", "repr": "gender: str = FEMALE", "required": true, "defaultValue": "FEMALE", "types": ["str"], "typedocs": []}, {"name": "encoding", "kind": "POSITIONAL_OR_NAMED", "repr": "encoding: str = MP3", "required": true, "defaultValue": "MP3", "types": ["str"], "typedocs": []}, {"name": "neural_voice_style", "kind": "POSITIONAL_OR_NAMED", "repr": "neural_voice_style: Any = None", "required": true, "defaultValue": "None", "types": ["Any"], "typedocs": []}, {"name": "target_file", "kind": "POSITIONAL_OR_NAMED", "repr": "target_file: str = synthesized.mp3", "required": true, "defaultValue": "synthesized.mp3", "types": ["str"], "typedocs": []}], "doc": "Synthesize speech synchronously\n\nparam text\n\n:   input text to synthesize\n\nparam language\n\n:   voice language, defaults to \"en-US\"\n\nparam name\n\n:   voice name, defaults to \"en-US-AriaRUS\"\n\nparam gender\n\n:   voice gender, defaults to \"FEMALE\"\n\nparam encoding\n\n:   result encoding type, defaults to \"MP3\"\n\nparam neural_voice_style\n\n:   if given then neural voice is used,\n    example style. \"cheerful\"\n\nparam target_file\n\n:   save synthesized output to file,\n    defaults to \"synthesized.mp3\"\n\nreturn\n\n:   synthesized output in bytes\n\nNeural voices are only supported for Speech resources created in\nEast US, South East Asia, and West Europe regions.\n", "tags": [], "source": null, "shortdoc": "Synthesize speech synchronously", "lineno": 490}, {"name": "Vision Analyze", "args": [{"name": "image_file", "kind": "POSITIONAL_OR_NAMED", "repr": "image_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "image_url", "kind": "POSITIONAL_OR_NAMED", "repr": "image_url: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "visual_features", "kind": "POSITIONAL_OR_NAMED", "repr": "visual_features: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Identify features in the image\n\nparam image_file\n\n:   filepath of image file\n\nparam image_url\n\n:   URI to image, if given will be used instead of image\\_file\n\nparam visual_features\n\n:   comma separated list of features,\n    for example. \"Categories,Description,Color\"\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n\nSee [Computer Vision API](https://westcentralus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-ga) for valid feature names and their explanations:\n\n* Adult\n\n* Brands\n\n* Categories\n\n* Color\n\n* Description\n\n* Faces\n\n* ImageType\n\n* Objects\n\n* Tags\n", "tags": [], "source": null, "shortdoc": "Identify features in the image", "lineno": 371}, {"name": "Vision Describe", "args": [{"name": "image_file", "kind": "POSITIONAL_OR_NAMED", "repr": "image_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "image_url", "kind": "POSITIONAL_OR_NAMED", "repr": "image_url: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Describe image with tags and captions\n\nparam image_file\n\n:   filepath of image file\n\nparam image_url\n\n:   URI to image, if given will be used instead of image\\_file\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n", "tags": [], "source": null, "shortdoc": "Describe image with tags and captions", "lineno": 412}, {"name": "Vision Detect Objects", "args": [{"name": "image_file", "kind": "POSITIONAL_OR_NAMED", "repr": "image_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "image_url", "kind": "POSITIONAL_OR_NAMED", "repr": "image_url: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Detect objects in the image\n\nparam image_file\n\n:   filepath of image file\n\nparam image_url\n\n:   URI to image, if given will be used instead of image\\_file\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n", "tags": [], "source": null, "shortdoc": "Detect objects in the image", "lineno": 448}, {"name": "Vision Ocr", "args": [{"name": "image_file", "kind": "POSITIONAL_OR_NAMED", "repr": "image_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "image_url", "kind": "POSITIONAL_OR_NAMED", "repr": "image_url: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}, {"name": "json_file", "kind": "POSITIONAL_OR_NAMED", "repr": "json_file: str = None", "required": true, "defaultValue": "None", "types": ["str"], "typedocs": []}], "doc": "Optical Character Recognition (OCR) detects text in an image\n\nparam image_file\n\n:   filepath of image file\n\nparam image_url\n\n:   URI to image, if given will be used instead of image\\_file\n\nparam json_file\n\n:   filepath to write results into\n\nreturn\n\n:   analysis in json format\n", "tags": [], "source": null, "shortdoc": "Optical Character Recognition (OCR) detects text in an image", "lineno": 430}], "dataTypes": {}, "typedocs": []}